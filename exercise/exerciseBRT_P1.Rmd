---
title: "Exercise complete"
author: "Olalla Díaz-Yáñez"
date: "2023-06-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

We are assuming that you sampled many plots and you want to understand what is according to the observations the most suitable habitat for two selected species of birds. The resulting collected database is $observations.csv$.

You can download the data in here:

LINK TO THE FOLDER TO BE ADDED

You can load the data by doing:

```{r load data}

observations <- readxl::read_excel(here::here("data/observations.xlsx"))

```

You should now explore the dataset, understand each of the variables and their values.

You can start by doing the following:

```{r data mining}
View(observations)
summary(observations)
head(observations)
```

## The data

The data collects observations of presence /absence of certain species across 100 plots measured in Czech Republic (DATA SOURCE). The predictor variable is the number of observed species of the taxon per plot.

## Species description

### Species 1 - Bryophytes

Bryophytes constitute an important and permanent component of the forest flora and diversity. They colonize various substrates, which are unsuitable for vascular plants, because of low light intensity or low nutrient level, such as deadwood, bark, rocks, and open soil. They provide shelter habitats, food, and nest material for many animals.

In forests, different ecological guilds of bryophytes can be distinguished by the substrate on which they are growing, including terricolous, lignicolous, corticolous and saxicolous species that occur on soil, deadwood, bark of living trees and shrubs, or rocks, respectively. As diversity and quality of these substrates is affected by forest management, bryophytes are suitable indicators for the effect of management on forest conditions. Especially typical woodland bryophytes, which are strictly depending on forest conditions.

Some studies dealt with forest management effects on bryophytes and demonstrated their sensitivity to management practices. However, comparative studies on bryophyte diversity in forest ecosystems were mostly restricted to boreal regions and only compared unmanaged with managed forests.

<https://www.sciencedirect.com/science/article/abs/pii/S0378112718312866>

### Species 2



## Models description

### GLM

We use a generalized linear model (GLM) to understand the abundance of different bryophites species at the 99 plots. Count data often conform to a Poisson distribution, in this case we have a count of the number of species recorded at each plot.

Fitting a Poisson GLM in R is very similar to fitting an analysis of covariance (or linear model), except that now we need to use the glm function. To run a GLM, we need to provide one extra piece of information beyond that needed for a linear model: the family of model we want to use. In this case, we want a Poisson family, family=poisson.




### Model behaviour {#modelsDes}

All models were evaluated quantitatively by examining the magnitude and distribution of residuals for all possible combinations of predictors included in the models. The aim was to detect any obvious patterns that indicate systematic discrepancies. The residual deviance was used for model checking during the models' creation. To determine the goodness-of-fit of the models the Hosmer-Lemeshow statistic was calculated (Hosmer et al., 2013) and to assess the discrimination of the fitted model the receiver operating characteristic (ROC) curve and the area under the curve (AUC) were calculated. The models were fitted using the R software (R Development Core Team, 2008).

### Boosted regression trees (BRT)

Boosted regression trees (BRT) are a combination of two powerful statistical techniques: boosting and regression trees. Boosting is a machine learning technique similar to model averaging, where the results of several competing models are merged. Unlike model averaging, however, boosting uses a forward, stage-wise procedure, where tree models are fitted iteratively to a subset of the training data. Subsets of the training data used at each iteration of the model fit are randomly selected without replacement, where the proportion of the training data used is determined by the modeler, this is defined with the "bag fraction" parameter. This procedure, known as stochastic gradient boosting, introduces an element of stochasticity that improves model accuracy and reduces overfitting (Elith et al., 2008).

The BRT model calibration is defined by four parameters:

-   the ```learning rate``` (or shrinkage parameter): The learning rate determines the contribution of each new tree to the growing model, and it is always substantially lower than 1, higher values being related to faster learning.

-   the ```bag fraction```: The bag fraction provides in- formation on which fraction of the entire data should be drawn randomly to fit the new tree. This parameter includes a random probabilistic component, making each run model different, and is aimed at improving model accuracy, speed of model creation, and the reduction of overfitting (Friedman, 2002).

-   the ```tree complexity```: Tree complexity controls the number of fitted interactions among variables, and determines the number of splits in each tree; for example, a value of 1 will present only one split, meaning that the model does not consider interactions; a value of 2 will result in two splits, and two interactions.

-   The ```number of trees``` required for optimal prediction: The optimal number of trees is selected based on the three previous parameters. The values fitted by the final model are computed as the sum of all of the predictions of the trees, multiplied by their respective learning rates.

In this example we are going to use the default paremeters for the calibration, where learning rate= 0.01 and tree complexity= 1 and cross-validation= 10-fold. However, the bag fraction is changed from the default value, 0.75, to 0.5. As a family we used the Bernoulli family, because we are predicting presence/absence per plot.

## Questions:

### Group 1 -Bryophites diversity - GLM

#### Subgroup 3

#### Subgroup 4

### Group 2 - Bryophites diversity - BRT

#### Subgroup 3

#### Subgroup 4

### Group 3 - Dendrocopos major - GLM

#### Subgroup 3

#### Subgroup 4

### Group 4 - Dendrocopos major - BRT

#### Subgroup 3

#### Subgroup 4

#### Response variable

In this case we want to assess the occurrence of certain species across the plots. In other words, we want to assess what is the probability of a certain species with biodiversity interest to be present in a plot based on the variables that describe the forest of that plot. In this case the predictor variable is:

-   $dendrocoposMajor$ is the response variable and represent if the Dendrocopos major has been observed in this plot or not.


#### Explanatory variables

First you need to select the variables of interest, after you have explored the data you can decide which variables you want to use to fit the model.

You can create a vector $selVar$ in which you add the names of the selected variables. Then you only take those variables from the data that you will use to create the model.

We are proposing to select the following variables:

-   $latitude$ as a proxy for climate

-   $forestManagementType$ to assess if different management types have different impact in the presence / absence of Parus major.

-   $volAllha$ that is the total volume in the plot, as a proxy of how dense the plot is. Higher volumes will mean that the forest is more dense.

-   $Fagus sylvatica$ and $Picea abies$ variables representing the proportion of the volume that this two species represent in that plot.

-   $GiniDBH$ showing how homogeneous the plot is in trees diameters. A value closer to 1 will mean that indicate more structural heterogeneity, lower values indicate more homogeneous plots.

-   $Tracheophyta_rich$, $Birds_rich$, $Bryophytes_rich$, $Fungi_rich$, $Lichens_rich$ and $Beetles_rich$ are the species richness value calculated for that species, higher value indicate higher species richness for that taxon.

```{r selectVar}
# Select variables from the dataset for the model
selVar <- c("dendrocoposMajor", "latitude","forestManagementType", 
            "forestStructure","Fagus sylvatica", "Picea abies", "volAllha",
            "GiniDBH",  "ShannonIndexTreeSpp", "Tracheophyta_rich",
            "Birds_rich", "Bryophytes_rich", "Fungi_rich", "Lichens_rich",
            "Beetles_rich")

# Filter the dataset to the selected variables
modelDataSel <- modelData[, colnames(modelData) %in% selVar]

modelDataSel <- rbind(modelDataSel, modelDataSel, modelDataSel, modelDataSel,
                      modelDataSel)

```

Now it is important to assess if the variables have the right categories. Variables should be type numeric or factor. 

```{r check data}
summary(modelDataSel)

# Two variables are character, we assign to factor instead:
modelDataSel$forestManagementType <- as.factor(modelDataSel$forestManagementType)
modelDataSel$forestStructure <- as.factor(modelDataSel$forestStructure)

```

In the next step you can see how you can run the model with the selected variables and model parameters. You have a description of the models parameters in the section Model description. 

```{r BRTmodel}
family <- "bernoulli"

library(gbm)
library(dismo)

    tc = 1    # tree complexity
    lr = 0.01 # learning rate-shrinkage 
    bag = 0.5 # bag fraction
    
    modelBRT <- gbm.step(data = modelDataSel,
                          gbm.x = selVar[2:length(selVar)], #indices or names of predictor variables in data
                          gbm.y = selVar[1], #index or name of response variable in data
                          family = family,
                          tree.complexity = tc,
                          learning.rate = lr,
                          bag.fraction = bag)  

```

#### Model behaviour

##### Model parameters

You can summarized the model parameters used and the cross valiadation statistics from the fitted model:

```{r BRTeval}

 results <- data.frame(

# Model parameters
Tree.Complexity = modelBRT$gbm.call$tree.complexity, 
Learning.Rate = modelBRT$gbm.call$learning.rate, 
                 Bag.Fraction = modelBRT$gbm.call$bag.fraction,
                 Interaction.depth = modelBRT$interaction.depth,
                 Shrinkage = modelBRT$shrinkage,
                 N.trees = modelBRT$n.trees,
     
# Cross validation statistics 
## Self statistics 
# It demonstrates ”evaluation” (i.e. fit) on the training data
                 
#modelBRT$self.statistics$mean.null,          
## mean total deviance
Deviance = modelBRT$self.statistics$mean.resid, # mean residual deviance 

AUC = modelBRT$self.statistics$discrimination, # training data AUC score

Corr = modelBRT$self.statistics$correlation,   # training data correlation
                 
## Cross Validation statistics
                 
# We calculate each statistic within each fold (at the identified optimal number
# of trees that is calculated on the mean change in predictive deviance over all folds), 
#then present here the mean and standard error of those fold-based statistics.
                 
devianceCV = modelBRT$cv.statistics$deviance.mean,  # estimated cv deviance 
devianceCVse = modelBRT$cv.statistics$deviance.se,  # estimated cv deviance se
                 
CorrCV = modelBRT$cv.statistics$correlation.mean,  #cv correlation
CorrCVse = modelBRT$cv.statistics$correlation.se,  #cv correlation se
                 
AUCcv = modelBRT$cv.statistics$discrimination.mean, # cv AUC score
AUCcvSE = modelBRT$cv.statistics$discrimination.se,  # cv AUC score se
                 
RMSD = rmsd(modelBRT$fitted, average.pred),
RMSE = rmse(modelBRT$gbm.call$dataframe$y, modelBRT$fitted)) #Root Mean Squared Error predictions against obcerved data 
    

```

##### Model outputs analysis

Before that we can check the most important variables:

```{r varImportance}
# Variables contribution or weighs in all the models----

# Selection of variables with a contirbution higer thant 5%
table.contrib <- modelBRT$contributions 


# Marginal dependency values   ----                       

Val.Marg.Dep <- function(modelBRT){
    
    Val.Marg.Dep.list <- vector("list",length(modelBRT$contributions$var))
    names(Val.Marg.Dep.list) <- modelBRT$contributions$var
    
    
    for (j in 1:length(modelBRT$contributions$var)){
        
        k <- match(modelBRT$contributions$var[j],
                   modelBRT$gbm.call$predictor.names)
        
        response.matrix <- gbm::plot.gbm(modelBRT, k, return.grid = TRUE)
        predictors <- response.matrix[,1]   
        
        if ( is.factor(predictors) == TRUE){
            #If factor variable
            predictors <- factor(predictors,
                                 levels = levels(modelBRT$gbm.call$dataframe[,modelBRT$gbm.call$gbm.x[k]]))
        }
        
        responses <- response.matrix[,2] - mean(response.matrix[,2])  
        
        #Perform transformations
        #Put it on a log scale
        responses <- 1/(1+exp(-responses))
        
        #Center the response to have zero mean over the data distribution
        responses <- scale(responses, scale = FALSE)
    
        
        
            
        # Explanation:
        # log simply takes the logarithm (base e, by default) of each element 
        # of the vector. scale, with default settings, 
        # will calculate the mean and standard deviation of the entire vector,
        # then "scale" each element by those values by subtracting the mean 
        # and dividing by the sd. (If you use  scale(x, scale=FALSE), 
        # it will only subtract the mean but not divide by the std deviation.)
        
        
        #Add a smoothed line to an active plot
        #loess(responses[[j]] ~ predictors[[j]], span = 0.3)
        Val.Marg.Dep.list [[j]] <- data.frame (predictors,responses)
    }
    return( Val.Marg.Dep.list)
}

#get all the variables values of the marginal dependency from all the models

Val.Marg.Dep.list <- lapply (list.models, Val.Marg.Dep) 

save (Val.Marg.Dep.list, file=file.path(ResultsDir,
                                        file="Val.Marg.Dep.list.rdata"))

```

We can evaluate the model behavior via partial dependence plots, showing the effect of each of the variables on the response by accounting for the average effects of all other predictors in the model.

```{r marginallEffects}
    gbm.plot(modelBRT, n.plots=16,  
             plot.layout = c(6, 3), write.title = F)

# If you want to save this plot you can also run:
png(filename=file.path(here::here(), 
                       file= "BRT_margEffPlot.png"),
        width     = 15,
        height    = 24,
        units     = "cm",
        res       = 300,
        pointsize = 11)
    
    gbm.plot(modelBRT, n.plots = 16,  
             plot.layout = c(6, 3), write.title = F)
    dev.off()
```

In order to identify the fitted interaction effects, you can test the partial dependencies between the response and predictor variables. The model predictions can be obtained for each pair of predictor variables, setting all other predictors to their means.

We plotted the most significant pairwise interactions -- those variables with higher relative significance.

